{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa924e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dd9f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer import SGD, AdaGrad, AdaDelta\n",
    "from torch.nn import Parameter\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05cb581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x: torch.Tensor) -> torch.Tensor:\n",
    "    return (x-3) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d52ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, x: 0.6000000238418579, loss: 9.0000\n",
      "Step: 2, x: 1.0800000429153442, loss: 5.7600\n",
      "Step: 3, x: 1.4639999866485596, loss: 3.6864\n",
      "Step: 4, x: 1.7711999416351318, loss: 2.3593\n",
      "Step: 5, x: 2.0169599056243896, loss: 1.5099\n",
      "Step: 6, x: 2.2135679721832275, loss: 0.9664\n",
      "Step: 7, x: 2.370854377746582, loss: 0.6185\n",
      "Step: 8, x: 2.4966835975646973, loss: 0.3958\n",
      "Step: 9, x: 2.597346782684326, loss: 0.2533\n",
      "Step: 10, x: 2.677877426147461, loss: 0.1621\n",
      "Step: 11, x: 2.7423019409179688, loss: 0.1038\n",
      "Step: 12, x: 2.793841600418091, loss: 0.0664\n",
      "Step: 13, x: 2.835073232650757, loss: 0.0425\n",
      "Step: 14, x: 2.868058681488037, loss: 0.0272\n",
      "Step: 15, x: 2.894446849822998, loss: 0.0174\n",
      "Step: 16, x: 2.915557384490967, loss: 0.0111\n",
      "Step: 17, x: 2.932446002960205, loss: 0.0071\n",
      "Step: 18, x: 2.9459567070007324, loss: 0.0046\n",
      "Step: 19, x: 2.9567654132843018, loss: 0.0029\n",
      "Step: 20, x: 2.9654123783111572, loss: 0.0019\n",
      "Step: 21, x: 2.97232985496521, loss: 0.0012\n",
      "difference in loss is less than the threshold; stopping\n"
     ]
    }
   ],
   "source": [
    "x = Parameter(data=torch.tensor([0.0]), requires_grad=True)\n",
    "optimizer = SGD(lr=0.1, params=[x], momentum=None)\n",
    "\n",
    "loss_threshold = 1e-3\n",
    "prev_loss = 0\n",
    "\n",
    "for step in range(50):\n",
    "    loss = loss_fn(x)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Step: {step+1}, x: {x.item()}, loss: {loss.item():.4f}\")\n",
    "\n",
    "    if abs(prev_loss - loss.item()) < loss_threshold:\n",
    "        print(f\"difference in loss is less than the threshold; stopping\")\n",
    "        break\n",
    "    prev_loss = loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd512b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, x: 0.6000000238418579, loss: 9.0000\n",
      "Step: 2, x: 1.380000114440918, loss: 5.7600\n",
      "Step: 3, x: 2.0940001010894775, loss: 2.6244\n",
      "Step: 4, x: 2.632200002670288, loss: 0.8208\n",
      "Step: 5, x: 2.9748599529266357, loss: 0.1353\n",
      "Step: 6, x: 3.1512179374694824, loss: 0.0006\n",
      "Step: 7, x: 3.209153413772583, loss: 0.0229\n",
      "Step: 8, x: 3.1962904930114746, loss: 0.0437\n",
      "Step: 9, x: 3.1506009101867676, loss: 0.0385\n",
      "Step: 10, x: 3.0976359844207764, loss: 0.0227\n",
      "Step: 11, x: 3.051626205444336, loss: 0.0095\n",
      "Step: 12, x: 3.018296241760254, loss: 0.0027\n",
      "Step: 13, x: 2.997972011566162, loss: 0.0003\n",
      "Step: 14, x: 2.988215446472168, loss: 0.0000\n",
      "difference in loss is less than the threshold; stopping\n"
     ]
    }
   ],
   "source": [
    "x = Parameter(data=torch.tensor([0.0]), requires_grad=True)\n",
    "optimizer = SGD(lr=0.1, params=[x], momentum=0.5)\n",
    "\n",
    "loss_threshold = 1e-3\n",
    "prev_loss = 0\n",
    "\n",
    "for step in range(50):\n",
    "    loss = loss_fn(x)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Step: {step+1}, x: {x.item()}, loss: {loss.item():.4f}\")\n",
    "\n",
    "    if abs(prev_loss - loss.item()) < loss_threshold:\n",
    "        print(f\"difference in loss is less than the threshold; stopping\")\n",
    "        break\n",
    "    prev_loss = loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577e9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, x: 1.0, loss: 9.0000\n",
      "Step: 2, x: 1.5547001361846924, loss: 4.0000\n",
      "Step: 3, x: 1.926774024963379, loss: 2.0889\n",
      "Step: 4, x: 2.193084716796875, loss: 1.1518\n",
      "Step: 5, x: 2.389416217803955, loss: 0.6511\n",
      "Step: 6, x: 2.536365270614624, loss: 0.3728\n",
      "Step: 7, x: 2.6472599506378174, loss: 0.2150\n",
      "Step: 8, x: 2.7313315868377686, loss: 0.1244\n",
      "Step: 9, x: 2.7952346801757812, loss: 0.0722\n",
      "Step: 10, x: 2.8438806533813477, loss: 0.0419\n",
      "Step: 11, x: 2.8809444904327393, loss: 0.0244\n",
      "Step: 12, x: 2.9091978073120117, loss: 0.0142\n",
      "Step: 13, x: 2.930741310119629, loss: 0.0082\n",
      "Step: 14, x: 2.947171211242676, loss: 0.0048\n",
      "Step: 15, x: 2.959702491760254, loss: 0.0028\n",
      "Step: 16, x: 2.9692609310150146, loss: 0.0016\n",
      "Step: 17, x: 2.9765520095825195, loss: 0.0009\n",
      "difference in loss is less than the threshold; stopping\n"
     ]
    }
   ],
   "source": [
    "x = Parameter(data=torch.tensor([0.0]), requires_grad=True)\n",
    "optimizer = AdaGrad(lr=1, params=[x])\n",
    "\n",
    "loss_threshold = 1e-3\n",
    "prev_loss = 0\n",
    "\n",
    "for step in range(50):\n",
    "    loss = loss_fn(x)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Step: {step+1}, x: {x.item()}, loss: {loss.item():.4f}\")\n",
    "\n",
    "    if abs(prev_loss - loss.item()) < loss_threshold:\n",
    "        print(f\"difference in loss is less than the threshold; stopping\")\n",
    "        break\n",
    "    prev_loss = loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ff380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, x: 3.163859946653247e-05, loss: 9.0000\n",
      "Step: 2, x: 7.635998190380633e-05, loss: 8.9998\n",
      "Step: 3, x: 0.00013112311717122793, loss: 8.9995\n",
      "Step: 4, x: 0.00019435286230873317, loss: 8.9992\n",
      "Step: 5, x: 0.00026504232664592564, loss: 8.9988\n",
      "Step: 6, x: 0.0003424761525820941, loss: 8.9984\n",
      "Step: 7, x: 0.00042611226672306657, loss: 8.9979\n",
      "Step: 8, x: 0.0005155213875696063, loss: 8.9974\n",
      "Step: 9, x: 0.0006103527848608792, loss: 8.9969\n",
      "Step: 10, x: 0.0007103127427399158, loss: 8.9963\n",
      "Step: 11, x: 0.0008151506772264838, loss: 8.9957\n",
      "Step: 12, x: 0.0009246495319530368, loss: 8.9951\n",
      "Step: 13, x: 0.001038618735037744, loss: 8.9945\n",
      "Step: 14, x: 0.0011568896006792784, loss: 8.9938\n",
      "Step: 15, x: 0.0012793110217899084, loss: 8.9931\n",
      "Step: 16, x: 0.0014057466760277748, loss: 8.9923\n",
      "Step: 17, x: 0.001536073163151741, loss: 8.9916\n",
      "Step: 18, x: 0.001670177560299635, loss: 8.9908\n",
      "Step: 19, x: 0.0018079562578350306, loss: 8.9900\n",
      "Step: 20, x: 0.0019493139116093516, loss: 8.9892\n",
      "Step: 21, x: 0.002094161929562688, loss: 8.9883\n",
      "Step: 22, x: 0.002242418471723795, loss: 8.9874\n",
      "Step: 23, x: 0.0023940065875649452, loss: 8.9866\n",
      "Step: 24, x: 0.002548854798078537, loss: 8.9856\n",
      "Step: 25, x: 0.0027068958152085543, loss: 8.9847\n",
      "Step: 26, x: 0.002868066541850567, loss: 8.9838\n",
      "Step: 27, x: 0.0030323071405291557, loss: 8.9828\n",
      "Step: 28, x: 0.0031995612662285566, loss: 8.9818\n",
      "Step: 29, x: 0.003369775600731373, loss: 8.9808\n",
      "Step: 30, x: 0.0035428996197879314, loss: 8.9798\n",
      "Step: 31, x: 0.0037188853602856398, loss: 8.9788\n",
      "Step: 32, x: 0.0038976867217570543, loss: 8.9777\n",
      "Step: 33, x: 0.0040792603977024555, loss: 8.9766\n",
      "Step: 34, x: 0.004263564478605986, loss: 8.9755\n",
      "Step: 35, x: 0.004450558684766293, loss: 8.9744\n",
      "Step: 36, x: 0.004640205297619104, loss: 8.9733\n",
      "Step: 37, x: 0.004832467995584011, loss: 8.9722\n",
      "Step: 38, x: 0.005027311388403177, loss: 8.9710\n",
      "Step: 39, x: 0.0052247014828026295, loss: 8.9699\n",
      "Step: 40, x: 0.0054246061481535435, loss: 8.9687\n",
      "Step: 41, x: 0.00562699418514967, loss: 8.9675\n",
      "Step: 42, x: 0.005831835325807333, loss: 8.9663\n",
      "Step: 43, x: 0.00603910069912672, loss: 8.9650\n",
      "Step: 44, x: 0.006248761899769306, loss: 8.9638\n",
      "Step: 45, x: 0.006460792385041714, loss: 8.9625\n",
      "Step: 46, x: 0.0066751656122505665, loss: 8.9613\n",
      "Step: 47, x: 0.00689185643568635, loss: 8.9600\n",
      "Step: 48, x: 0.0071108401753008366, loss: 8.9587\n",
      "Step: 49, x: 0.007332093082368374, loss: 8.9574\n",
      "Step: 50, x: 0.007555591873824596, loss: 8.9561\n",
      "Step: 51, x: 0.007781314663589001, loss: 8.9547\n",
      "Step: 52, x: 0.00800924003124237, loss: 8.9534\n",
      "Step: 53, x: 0.008239345625042915, loss: 8.9520\n",
      "Step: 54, x: 0.008471611887216568, loss: 8.9506\n",
      "Step: 55, x: 0.008706019259989262, loss: 8.9492\n",
      "Step: 56, x: 0.00894254818558693, loss: 8.9478\n",
      "Step: 57, x: 0.009181179106235504, loss: 8.9464\n",
      "Step: 58, x: 0.009421894326806068, loss: 8.9450\n",
      "Step: 59, x: 0.00966467522084713, loss: 8.9436\n",
      "Step: 60, x: 0.009909505024552345, loss: 8.9421\n",
      "Step: 61, x: 0.010156366974115372, loss: 8.9406\n",
      "Step: 62, x: 0.010405244305729866, loss: 8.9392\n",
      "Step: 63, x: 0.010656120255589485, loss: 8.9377\n",
      "Step: 64, x: 0.01090897899121046, loss: 8.9362\n",
      "Step: 65, x: 0.011163805611431599, loss: 8.9347\n",
      "Step: 66, x: 0.011420585215091705, loss: 8.9331\n",
      "Step: 67, x: 0.011679302901029587, loss: 8.9316\n",
      "Step: 68, x: 0.01193994376808405, loss: 8.9301\n",
      "Step: 69, x: 0.012202493846416473, loss: 8.9285\n",
      "Step: 70, x: 0.012466940097510815, loss: 8.9269\n",
      "Step: 71, x: 0.01273326762020588, loss: 8.9254\n",
      "Step: 72, x: 0.013001464307308197, loss: 8.9238\n",
      "Step: 73, x: 0.013271517120301723, loss: 8.9222\n",
      "Step: 74, x: 0.013543413020670414, loss: 8.9205\n",
      "Step: 75, x: 0.013817139901220798, loss: 8.9189\n",
      "Step: 76, x: 0.014092685654759407, loss: 8.9173\n",
      "Step: 77, x: 0.01437003817409277, loss: 8.9156\n",
      "Step: 78, x: 0.01464918628334999, loss: 8.9140\n",
      "Step: 79, x: 0.0149301178753376, loss: 8.9123\n",
      "Step: 80, x: 0.015212821774184704, loss: 8.9106\n",
      "Step: 81, x: 0.015497286804020405, loss: 8.9090\n",
      "Step: 82, x: 0.01578350178897381, loss: 8.9073\n",
      "Step: 83, x: 0.016071457415819168, loss: 8.9055\n",
      "Step: 84, x: 0.016361141577363014, loss: 8.9038\n",
      "Step: 85, x: 0.0166525449603796, loss: 8.9021\n",
      "Step: 86, x: 0.01694565825164318, loss: 8.9004\n",
      "Step: 87, x: 0.01724047027528286, loss: 8.8986\n",
      "Step: 88, x: 0.01753697171807289, loss: 8.8969\n",
      "Step: 89, x: 0.01783515326678753, loss: 8.8951\n",
      "Step: 90, x: 0.018135005608201027, loss: 8.8933\n",
      "Step: 91, x: 0.01843651942908764, loss: 8.8915\n",
      "Step: 92, x: 0.01873968541622162, loss: 8.8897\n",
      "Step: 93, x: 0.01904449425637722, loss: 8.8879\n",
      "Step: 94, x: 0.019350936636328697, loss: 8.8861\n",
      "Step: 95, x: 0.019659005105495453, loss: 8.8843\n",
      "Step: 96, x: 0.01996869035065174, loss: 8.8824\n",
      "Step: 97, x: 0.020279984921216965, loss: 8.8806\n",
      "Step: 98, x: 0.020592879503965378, loss: 8.8787\n",
      "Step: 99, x: 0.020907366648316383, loss: 8.8769\n",
      "Step: 100, x: 0.021223437041044235, loss: 8.8750\n",
      "Step: 101, x: 0.021541085094213486, loss: 8.8731\n",
      "Step: 102, x: 0.02186030149459839, loss: 8.8712\n",
      "Step: 103, x: 0.022181078791618347, loss: 8.8693\n",
      "Step: 104, x: 0.022503409534692764, loss: 8.8674\n",
      "Step: 105, x: 0.022827286273241043, loss: 8.8655\n",
      "Step: 106, x: 0.023152701556682587, loss: 8.8636\n",
      "Step: 107, x: 0.023479647934436798, loss: 8.8616\n",
      "Step: 108, x: 0.02380811795592308, loss: 8.8597\n",
      "Step: 109, x: 0.024138106033205986, loss: 8.8577\n",
      "Step: 110, x: 0.02446960285305977, loss: 8.8558\n",
      "Step: 111, x: 0.02480260282754898, loss: 8.8538\n",
      "Step: 112, x: 0.025137100368738174, loss: 8.8518\n",
      "Step: 113, x: 0.025473088026046753, loss: 8.8498\n",
      "Step: 114, x: 0.02581055834889412, loss: 8.8478\n",
      "Step: 115, x: 0.026149505749344826, loss: 8.8458\n",
      "Step: 116, x: 0.026489924639463425, loss: 8.8438\n",
      "Step: 117, x: 0.02683180756866932, loss: 8.8418\n",
      "Step: 118, x: 0.027175147086381912, loss: 8.8397\n",
      "Step: 119, x: 0.027519939467310905, loss: 8.8377\n",
      "Step: 120, x: 0.027866177260875702, loss: 8.8356\n",
      "Step: 121, x: 0.028213854879140854, loss: 8.8336\n",
      "Step: 122, x: 0.028562964871525764, loss: 8.8315\n",
      "Step: 123, x: 0.028913503512740135, loss: 8.8294\n",
      "Step: 124, x: 0.02926546335220337, loss: 8.8274\n",
      "Step: 125, x: 0.029618840664625168, loss: 8.8253\n",
      "Step: 126, x: 0.029973627999424934, loss: 8.8232\n",
      "Step: 127, x: 0.03032981976866722, loss: 8.8211\n",
      "Step: 128, x: 0.03068741224706173, loss: 8.8189\n",
      "Step: 129, x: 0.031046397984027863, loss: 8.8168\n",
      "Step: 130, x: 0.03140677139163017, loss: 8.8147\n",
      "Step: 131, x: 0.03176853060722351, loss: 8.8125\n",
      "Step: 132, x: 0.03213166818022728, loss: 8.8104\n",
      "Step: 133, x: 0.03249617666006088, loss: 8.8082\n",
      "Step: 134, x: 0.03286205232143402, loss: 8.8061\n",
      "Step: 135, x: 0.033229291439056396, loss: 8.8039\n",
      "Step: 136, x: 0.03359789028763771, loss: 8.8017\n",
      "Step: 137, x: 0.033967841416597366, loss: 8.7995\n",
      "Step: 138, x: 0.03433913737535477, loss: 8.7973\n",
      "Step: 139, x: 0.03471177816390991, loss: 8.7951\n",
      "Step: 140, x: 0.035085756331682205, loss: 8.7929\n",
      "Step: 141, x: 0.03546106815338135, loss: 8.7907\n",
      "Step: 142, x: 0.03583770990371704, loss: 8.7885\n",
      "Step: 143, x: 0.03621567413210869, loss: 8.7863\n",
      "Step: 144, x: 0.03659495711326599, loss: 8.7840\n",
      "Step: 145, x: 0.03697555512189865, loss: 8.7818\n",
      "Step: 146, x: 0.03735746443271637, loss: 8.7795\n",
      "Step: 147, x: 0.03774067759513855, loss: 8.7773\n",
      "Step: 148, x: 0.03812519088387489, loss: 8.7750\n",
      "Step: 149, x: 0.0385110005736351, loss: 8.7727\n",
      "Step: 150, x: 0.038898102939128876, loss: 8.7704\n",
      "Step: 151, x: 0.03928649425506592, loss: 8.7681\n",
      "Step: 152, x: 0.03967617079615593, loss: 8.7658\n",
      "Step: 153, x: 0.040067125111818314, loss: 8.7635\n",
      "Step: 154, x: 0.04045935720205307, loss: 8.7612\n",
      "Step: 155, x: 0.0408528596162796, loss: 8.7589\n",
      "Step: 156, x: 0.04124762862920761, loss: 8.7566\n",
      "Step: 157, x: 0.0416436605155468, loss: 8.7542\n",
      "Step: 158, x: 0.042040951550006866, loss: 8.7519\n",
      "Step: 159, x: 0.042439498007297516, loss: 8.7495\n",
      "Step: 160, x: 0.04283929616212845, loss: 8.7472\n",
      "Step: 161, x: 0.043240342289209366, loss: 8.7448\n",
      "Step: 162, x: 0.04364263266324997, loss: 8.7424\n",
      "Step: 163, x: 0.04404616355895996, loss: 8.7400\n",
      "Step: 164, x: 0.04445092752575874, loss: 8.7377\n",
      "Step: 165, x: 0.044856924563646317, loss: 8.7353\n",
      "Step: 166, x: 0.04526415094733238, loss: 8.7329\n",
      "Step: 167, x: 0.04567260295152664, loss: 8.7305\n",
      "Step: 168, x: 0.0460822731256485, loss: 8.7281\n",
      "Step: 169, x: 0.04649316146969795, loss: 8.7256\n",
      "Step: 170, x: 0.046905264258384705, loss: 8.7232\n",
      "Step: 171, x: 0.04731857776641846, loss: 8.7208\n",
      "Step: 172, x: 0.04773309826850891, loss: 8.7183\n",
      "Step: 173, x: 0.04814882203936577, loss: 8.7159\n",
      "Step: 174, x: 0.04856574535369873, loss: 8.7134\n",
      "Step: 175, x: 0.0489838644862175, loss: 8.7110\n",
      "Step: 176, x: 0.049403175711631775, loss: 8.7085\n",
      "Step: 177, x: 0.04982367530465126, loss: 8.7060\n",
      "Step: 178, x: 0.050245363265275955, loss: 8.7035\n",
      "Step: 179, x: 0.05066823214292526, loss: 8.7011\n",
      "Step: 180, x: 0.05109228193759918, loss: 8.6986\n",
      "Step: 181, x: 0.051517508924007416, loss: 8.6961\n",
      "Step: 182, x: 0.051943909376859665, loss: 8.6935\n",
      "Step: 183, x: 0.05237147957086563, loss: 8.6910\n",
      "Step: 184, x: 0.052800215780735016, loss: 8.6885\n",
      "Step: 185, x: 0.05323011428117752, loss: 8.6860\n",
      "Step: 186, x: 0.05366117134690285, loss: 8.6835\n",
      "Step: 187, x: 0.054093386977910995, loss: 8.6809\n",
      "Step: 188, x: 0.05452675744891167, loss: 8.6784\n",
      "Step: 189, x: 0.05496127903461456, loss: 8.6758\n",
      "Step: 190, x: 0.055396948009729385, loss: 8.6733\n",
      "Step: 191, x: 0.055833760648965836, loss: 8.6707\n",
      "Step: 192, x: 0.056271716952323914, loss: 8.6681\n",
      "Step: 193, x: 0.05671081319451332, loss: 8.6655\n",
      "Step: 194, x: 0.05715104565024376, loss: 8.6630\n",
      "Step: 195, x: 0.05759241059422493, loss: 8.6604\n",
      "Step: 196, x: 0.058034904301166534, loss: 8.6578\n",
      "Step: 197, x: 0.05847852677106857, loss: 8.6552\n",
      "Step: 198, x: 0.05892327427864075, loss: 8.6525\n",
      "Step: 199, x: 0.05936914309859276, loss: 8.6499\n",
      "Step: 200, x: 0.05981612950563431, loss: 8.6473\n",
      "Step: 201, x: 0.060264233499765396, loss: 8.6447\n",
      "Step: 202, x: 0.060713447630405426, loss: 8.6420\n",
      "Step: 203, x: 0.061163775622844696, loss: 8.6394\n",
      "Step: 204, x: 0.06161521002650261, loss: 8.6368\n",
      "Step: 205, x: 0.062067750841379166, loss: 8.6341\n",
      "Step: 206, x: 0.06252139061689377, loss: 8.6314\n",
      "Step: 207, x: 0.06297612935304642, loss: 8.6288\n",
      "Step: 208, x: 0.06343197077512741, loss: 8.6261\n",
      "Step: 209, x: 0.06388889998197556, loss: 8.6234\n",
      "Step: 210, x: 0.06434692442417145, loss: 8.6207\n",
      "Step: 211, x: 0.06480603665113449, loss: 8.6181\n",
      "Step: 212, x: 0.06526623666286469, loss: 8.6154\n",
      "Step: 213, x: 0.06572752445936203, loss: 8.6127\n",
      "Step: 214, x: 0.06618989259004593, loss: 8.6100\n",
      "Step: 215, x: 0.06665333360433578, loss: 8.6072\n",
      "Step: 216, x: 0.0671178549528122, loss: 8.6045\n",
      "Step: 217, x: 0.06758344918489456, loss: 8.6018\n",
      "Step: 218, x: 0.06805011630058289, loss: 8.5991\n",
      "Step: 219, x: 0.06851784884929657, loss: 8.5963\n",
      "Step: 220, x: 0.06898665428161621, loss: 8.5936\n",
      "Step: 221, x: 0.06945651769638062, loss: 8.5908\n",
      "Step: 222, x: 0.06992744654417038, loss: 8.5881\n",
      "Step: 223, x: 0.07039943337440491, loss: 8.5853\n",
      "Step: 224, x: 0.0708724781870842, loss: 8.5826\n",
      "Step: 225, x: 0.07134658098220825, loss: 8.5798\n",
      "Step: 226, x: 0.07182173430919647, loss: 8.5770\n",
      "Step: 227, x: 0.07229793816804886, loss: 8.5742\n",
      "Step: 228, x: 0.07277518510818481, loss: 8.5714\n",
      "Step: 229, x: 0.07325348258018494, loss: 8.5686\n",
      "Step: 230, x: 0.07373282313346863, loss: 8.5658\n",
      "Step: 231, x: 0.07421319931745529, loss: 8.5630\n",
      "Step: 232, x: 0.07469461858272552, loss: 8.5602\n",
      "Step: 233, x: 0.07517707347869873, loss: 8.5574\n",
      "Step: 234, x: 0.07566056400537491, loss: 8.5546\n",
      "Step: 235, x: 0.07614508271217346, loss: 8.5518\n",
      "Step: 236, x: 0.07663062959909439, loss: 8.5489\n",
      "Step: 237, x: 0.0771172046661377, loss: 8.5461\n",
      "Step: 238, x: 0.07760480791330338, loss: 8.5432\n",
      "Step: 239, x: 0.07809343189001083, loss: 8.5404\n",
      "Step: 240, x: 0.07858307659626007, loss: 8.5375\n",
      "Step: 241, x: 0.07907374203205109, loss: 8.5347\n",
      "Step: 242, x: 0.07956542819738388, loss: 8.5318\n",
      "Step: 243, x: 0.08005812764167786, loss: 8.5289\n",
      "Step: 244, x: 0.08055184036493301, loss: 8.5261\n",
      "Step: 245, x: 0.08104655891656876, loss: 8.5232\n",
      "Step: 246, x: 0.08154229074716568, loss: 8.5203\n",
      "Step: 247, x: 0.08203902840614319, loss: 8.5174\n",
      "Step: 248, x: 0.08253677189350128, loss: 8.5145\n",
      "Step: 249, x: 0.08303551375865936, loss: 8.5116\n",
      "Step: 250, x: 0.08353525400161743, loss: 8.5087\n",
      "Step: 251, x: 0.08403599262237549, loss: 8.5058\n",
      "Step: 252, x: 0.08453772962093353, loss: 8.5028\n",
      "Step: 253, x: 0.08504046499729156, loss: 8.4999\n",
      "Step: 254, x: 0.08554419130086899, loss: 8.4970\n",
      "Step: 255, x: 0.0860489085316658, loss: 8.4941\n",
      "Step: 256, x: 0.086554616689682, loss: 8.4911\n",
      "Step: 257, x: 0.087061308324337, loss: 8.4882\n",
      "Step: 258, x: 0.0875689834356308, loss: 8.4852\n",
      "Step: 259, x: 0.08807764202356339, loss: 8.4823\n",
      "Step: 260, x: 0.08858728408813477, loss: 8.4793\n",
      "Step: 261, x: 0.08909790217876434, loss: 8.4763\n",
      "Step: 262, x: 0.08960949629545212, loss: 8.4734\n",
      "Step: 263, x: 0.09012206643819809, loss: 8.4704\n",
      "Step: 264, x: 0.09063561260700226, loss: 8.4674\n",
      "Step: 265, x: 0.09115012735128403, loss: 8.4644\n",
      "Step: 266, x: 0.0916656106710434, loss: 8.4614\n",
      "Step: 267, x: 0.09218206256628036, loss: 8.4584\n",
      "Step: 268, x: 0.09269948303699493, loss: 8.4554\n",
      "Step: 269, x: 0.0932178646326065, loss: 8.4524\n",
      "Step: 270, x: 0.09373721480369568, loss: 8.4494\n",
      "Step: 271, x: 0.09425752609968185, loss: 8.4464\n",
      "Step: 272, x: 0.09477879106998444, loss: 8.4433\n",
      "Step: 273, x: 0.09530101716518402, loss: 8.4403\n",
      "Step: 274, x: 0.09582419693470001, loss: 8.4373\n",
      "Step: 275, x: 0.09634833037853241, loss: 8.4342\n",
      "Step: 276, x: 0.09687341749668121, loss: 8.4312\n",
      "Step: 277, x: 0.09739945083856583, loss: 8.4281\n",
      "Step: 278, x: 0.09792643785476685, loss: 8.4251\n",
      "Step: 279, x: 0.09845437109470367, loss: 8.4220\n",
      "Step: 280, x: 0.09898325055837631, loss: 8.4190\n",
      "Step: 281, x: 0.09951306879520416, loss: 8.4159\n",
      "Step: 282, x: 0.10004383325576782, loss: 8.4128\n",
      "Step: 283, x: 0.1005755364894867, loss: 8.4097\n",
      "Step: 284, x: 0.10110817849636078, loss: 8.4067\n",
      "Step: 285, x: 0.10164175927639008, loss: 8.4036\n",
      "Step: 286, x: 0.10217627137899399, loss: 8.4005\n",
      "Step: 287, x: 0.10271171480417252, loss: 8.3974\n",
      "Step: 288, x: 0.10324808955192566, loss: 8.3943\n",
      "Step: 289, x: 0.10378539562225342, loss: 8.3912\n",
      "Step: 290, x: 0.10432363301515579, loss: 8.3881\n",
      "Step: 291, x: 0.10486279428005219, loss: 8.3849\n",
      "Step: 292, x: 0.1054028868675232, loss: 8.3818\n",
      "Step: 293, x: 0.10594390332698822, loss: 8.3787\n",
      "Step: 294, x: 0.10648583620786667, loss: 8.3756\n",
      "Step: 295, x: 0.10702869296073914, loss: 8.3724\n",
      "Step: 296, x: 0.10757246613502502, loss: 8.3693\n",
      "Step: 297, x: 0.10811716318130493, loss: 8.3661\n",
      "Step: 298, x: 0.10866277664899826, loss: 8.3630\n",
      "Step: 299, x: 0.10920929908752441, loss: 8.3598\n",
      "Step: 300, x: 0.10975673794746399, loss: 8.3567\n",
      "Step: 301, x: 0.11030508577823639, loss: 8.3535\n",
      "Step: 302, x: 0.11085434257984161, loss: 8.3503\n",
      "Step: 303, x: 0.11140450835227966, loss: 8.3472\n",
      "Step: 304, x: 0.11195558309555054, loss: 8.3440\n",
      "Step: 305, x: 0.11250756680965424, loss: 8.3408\n",
      "Step: 306, x: 0.11306045204401016, loss: 8.3376\n",
      "Step: 307, x: 0.11361423879861832, loss: 8.3344\n",
      "Step: 308, x: 0.1141689270734787, loss: 8.3312\n",
      "Step: 309, x: 0.11472451686859131, loss: 8.3280\n",
      "Step: 310, x: 0.11528100073337555, loss: 8.3248\n",
      "Step: 311, x: 0.11583838611841202, loss: 8.3216\n",
      "Step: 312, x: 0.11639666557312012, loss: 8.3184\n",
      "Step: 313, x: 0.11695583909749985, loss: 8.3152\n",
      "Step: 314, x: 0.11751590669155121, loss: 8.3119\n",
      "Step: 315, x: 0.1180768609046936, loss: 8.3087\n",
      "Step: 316, x: 0.11863870918750763, loss: 8.3055\n",
      "Step: 317, x: 0.11920144408941269, loss: 8.3022\n",
      "Step: 318, x: 0.11976506561040878, loss: 8.2990\n",
      "Step: 319, x: 0.12032957375049591, loss: 8.2958\n",
      "Step: 320, x: 0.12089496105909348, loss: 8.2925\n",
      "Step: 321, x: 0.12146123498678207, loss: 8.2892\n",
      "Step: 322, x: 0.12202838808298111, loss: 8.2860\n",
      "Step: 323, x: 0.12259642034769058, loss: 8.2827\n",
      "Step: 324, x: 0.12316533178091049, loss: 8.2795\n",
      "Step: 325, x: 0.12373512238264084, loss: 8.2762\n",
      "Step: 326, x: 0.12430578470230103, loss: 8.2729\n",
      "Step: 327, x: 0.12487732619047165, loss: 8.2696\n",
      "Step: 328, x: 0.12544973194599152, loss: 8.2663\n",
      "Step: 329, x: 0.12602300941944122, loss: 8.2630\n",
      "Step: 330, x: 0.12659716606140137, loss: 8.2597\n",
      "Step: 331, x: 0.12717218697071075, loss: 8.2564\n",
      "Step: 332, x: 0.12774807214736938, loss: 8.2531\n",
      "Step: 333, x: 0.12832483649253845, loss: 8.2498\n",
      "Step: 334, x: 0.12890246510505676, loss: 8.2465\n",
      "Step: 335, x: 0.12948095798492432, loss: 8.2432\n",
      "Step: 336, x: 0.1300603151321411, loss: 8.2399\n",
      "Step: 337, x: 0.13064052164554596, loss: 8.2366\n",
      "Step: 338, x: 0.13122159242630005, loss: 8.2332\n",
      "Step: 339, x: 0.13180352747440338, loss: 8.2299\n",
      "Step: 340, x: 0.13238631188869476, loss: 8.2266\n",
      "Step: 341, x: 0.1329699605703354, loss: 8.2232\n",
      "Step: 342, x: 0.13355445861816406, loss: 8.2199\n",
      "Step: 343, x: 0.1341398060321808, loss: 8.2165\n",
      "Step: 344, x: 0.13472601771354675, loss: 8.2132\n",
      "Step: 345, x: 0.13531307876110077, loss: 8.2098\n",
      "Step: 346, x: 0.13590098917484283, loss: 8.2064\n",
      "Step: 347, x: 0.13648974895477295, loss: 8.2031\n",
      "Step: 348, x: 0.1370793581008911, loss: 8.1997\n",
      "Step: 349, x: 0.13766980171203613, loss: 8.1963\n",
      "Step: 350, x: 0.1382610946893692, loss: 8.1929\n",
      "Step: 351, x: 0.13885323703289032, loss: 8.1895\n",
      "Step: 352, x: 0.1394462138414383, loss: 8.1862\n",
      "Step: 353, x: 0.14004004001617432, loss: 8.1828\n",
      "Step: 354, x: 0.1406347006559372, loss: 8.1794\n",
      "Step: 355, x: 0.14123021066188812, loss: 8.1760\n",
      "Step: 356, x: 0.1418265551328659, loss: 8.1726\n",
      "Step: 357, x: 0.14242373406887054, loss: 8.1692\n",
      "Step: 358, x: 0.14302174746990204, loss: 8.1657\n",
      "Step: 359, x: 0.1436205953359604, loss: 8.1623\n",
      "Step: 360, x: 0.1442202776670456, loss: 8.1589\n",
      "Step: 361, x: 0.14482079446315765, loss: 8.1555\n",
      "Step: 362, x: 0.14542214572429657, loss: 8.1520\n",
      "Step: 363, x: 0.14602431654930115, loss: 8.1486\n",
      "Step: 364, x: 0.14662732183933258, loss: 8.1452\n",
      "Step: 365, x: 0.14723114669322968, loss: 8.1417\n",
      "Step: 366, x: 0.14783580601215363, loss: 8.1383\n",
      "Step: 367, x: 0.14844128489494324, loss: 8.1348\n",
      "Step: 368, x: 0.1490475982427597, loss: 8.1314\n",
      "Step: 369, x: 0.14965473115444183, loss: 8.1279\n",
      "Step: 370, x: 0.15026268362998962, loss: 8.1245\n",
      "Step: 371, x: 0.15087145566940308, loss: 8.1210\n",
      "Step: 372, x: 0.1514810472726822, loss: 8.1175\n",
      "Step: 373, x: 0.15209145843982697, loss: 8.1141\n",
      "Step: 374, x: 0.1527026891708374, loss: 8.1106\n",
      "Step: 375, x: 0.1533147394657135, loss: 8.1071\n",
      "Step: 376, x: 0.15392760932445526, loss: 8.1036\n",
      "Step: 377, x: 0.1545412838459015, loss: 8.1001\n",
      "Step: 378, x: 0.15515577793121338, loss: 8.0966\n",
      "Step: 379, x: 0.15577107667922974, loss: 8.0931\n",
      "Step: 380, x: 0.15638719499111176, loss: 8.0896\n",
      "Step: 381, x: 0.15700411796569824, loss: 8.0861\n",
      "Step: 382, x: 0.1576218456029892, loss: 8.0826\n",
      "Step: 383, x: 0.1582403928041458, loss: 8.0791\n",
      "Step: 384, x: 0.1588597446680069, loss: 8.0756\n",
      "Step: 385, x: 0.15947990119457245, loss: 8.0721\n",
      "Step: 386, x: 0.16010086238384247, loss: 8.0686\n",
      "Step: 387, x: 0.16072262823581696, loss: 8.0650\n",
      "Step: 388, x: 0.1613451987504959, loss: 8.0615\n",
      "Step: 389, x: 0.16196855902671814, loss: 8.0580\n",
      "Step: 390, x: 0.16259272396564484, loss: 8.0544\n",
      "Step: 391, x: 0.163217693567276, loss: 8.0509\n",
      "Step: 392, x: 0.16384345293045044, loss: 8.0473\n",
      "Step: 393, x: 0.16447001695632935, loss: 8.0438\n",
      "Step: 394, x: 0.16509737074375153, loss: 8.0402\n",
      "Step: 395, x: 0.16572552919387817, loss: 8.0367\n",
      "Step: 396, x: 0.1663544774055481, loss: 8.0331\n",
      "Step: 397, x: 0.1669842153787613, loss: 8.0295\n",
      "Step: 398, x: 0.16761474311351776, loss: 8.0260\n",
      "Step: 399, x: 0.1682460755109787, loss: 8.0224\n",
      "Step: 400, x: 0.1688781976699829, loss: 8.0188\n",
      "Step: 401, x: 0.1695111095905304, loss: 8.0153\n",
      "Step: 402, x: 0.17014481127262115, loss: 8.0117\n",
      "Step: 403, x: 0.170779287815094, loss: 8.0081\n",
      "Step: 404, x: 0.1714145541191101, loss: 8.0045\n",
      "Step: 405, x: 0.1720506101846695, loss: 8.0009\n",
      "Step: 406, x: 0.17268745601177216, loss: 7.9973\n",
      "Step: 407, x: 0.1733250766992569, loss: 7.9937\n",
      "Step: 408, x: 0.1739634871482849, loss: 7.9901\n",
      "Step: 409, x: 0.174602672457695, loss: 7.9865\n",
      "Step: 410, x: 0.17524264752864838, loss: 7.9829\n",
      "Step: 411, x: 0.17588339745998383, loss: 7.9793\n",
      "Step: 412, x: 0.17652492225170135, loss: 7.9756\n",
      "Step: 413, x: 0.17716722190380096, loss: 7.9720\n",
      "Step: 414, x: 0.17781031131744385, loss: 7.9684\n",
      "Step: 415, x: 0.1784541755914688, loss: 7.9648\n",
      "Step: 416, x: 0.17909881472587585, loss: 7.9611\n",
      "Step: 417, x: 0.17974422872066498, loss: 7.9575\n",
      "Step: 418, x: 0.180390402674675, loss: 7.9538\n",
      "Step: 419, x: 0.18103735148906708, loss: 7.9502\n",
      "Step: 420, x: 0.18168507516384125, loss: 7.9465\n",
      "Step: 421, x: 0.1823335736989975, loss: 7.9429\n",
      "Step: 422, x: 0.18298283219337463, loss: 7.9392\n",
      "Step: 423, x: 0.18363286554813385, loss: 7.9356\n",
      "Step: 424, x: 0.18428367376327515, loss: 7.9319\n",
      "Step: 425, x: 0.18493524193763733, loss: 7.9283\n",
      "Step: 426, x: 0.1855875700712204, loss: 7.9246\n",
      "Step: 427, x: 0.18624067306518555, loss: 7.9209\n",
      "Step: 428, x: 0.18689453601837158, loss: 7.9172\n",
      "Step: 429, x: 0.1875491589307785, loss: 7.9136\n",
      "Step: 430, x: 0.1882045418024063, loss: 7.9099\n",
      "Step: 431, x: 0.1888606995344162, loss: 7.9062\n",
      "Step: 432, x: 0.18951761722564697, loss: 7.9025\n",
      "Step: 433, x: 0.19017529487609863, loss: 7.8988\n",
      "Step: 434, x: 0.19083371758460999, loss: 7.8951\n",
      "Step: 435, x: 0.19149290025234222, loss: 7.8914\n",
      "Step: 436, x: 0.19215284287929535, loss: 7.8877\n",
      "Step: 437, x: 0.19281354546546936, loss: 7.8840\n",
      "Step: 438, x: 0.19347500801086426, loss: 7.8803\n",
      "Step: 439, x: 0.19413721561431885, loss: 7.8766\n",
      "Step: 440, x: 0.19480018317699432, loss: 7.8729\n",
      "Step: 441, x: 0.1954638957977295, loss: 7.8691\n",
      "Step: 442, x: 0.19612836837768555, loss: 7.8654\n",
      "Step: 443, x: 0.1967935860157013, loss: 7.8617\n",
      "Step: 444, x: 0.19745956361293793, loss: 7.8580\n",
      "Step: 445, x: 0.19812628626823425, loss: 7.8542\n",
      "Step: 446, x: 0.19879375398159027, loss: 7.8505\n",
      "Step: 447, x: 0.19946196675300598, loss: 7.8468\n",
      "Step: 448, x: 0.20013092458248138, loss: 7.8430\n",
      "Step: 449, x: 0.20080062747001648, loss: 7.8393\n",
      "Step: 450, x: 0.20147109031677246, loss: 7.8355\n",
      "Step: 451, x: 0.20214228332042694, loss: 7.8318\n",
      "Step: 452, x: 0.2028142213821411, loss: 7.8280\n",
      "Step: 453, x: 0.20348690450191498, loss: 7.8242\n",
      "Step: 454, x: 0.20416033267974854, loss: 7.8205\n",
      "Step: 455, x: 0.20483450591564178, loss: 7.8167\n",
      "Step: 456, x: 0.20550940930843353, loss: 7.8130\n",
      "Step: 457, x: 0.20618505775928497, loss: 7.8092\n",
      "Step: 458, x: 0.2068614512681961, loss: 7.8054\n",
      "Step: 459, x: 0.20753857493400574, loss: 7.8016\n",
      "Step: 460, x: 0.20821644365787506, loss: 7.7978\n",
      "Step: 461, x: 0.20889504253864288, loss: 7.7941\n",
      "Step: 462, x: 0.2095743715763092, loss: 7.7903\n",
      "Step: 463, x: 0.21025444567203522, loss: 7.7865\n",
      "Step: 464, x: 0.21093524992465973, loss: 7.7827\n",
      "Step: 465, x: 0.21161678433418274, loss: 7.7789\n",
      "Step: 466, x: 0.21229904890060425, loss: 7.7751\n",
      "Step: 467, x: 0.21298204362392426, loss: 7.7713\n",
      "Step: 468, x: 0.21366576850414276, loss: 7.7675\n",
      "Step: 469, x: 0.21435022354125977, loss: 7.7637\n",
      "Step: 470, x: 0.21503540873527527, loss: 7.7598\n",
      "Step: 471, x: 0.21572132408618927, loss: 7.7560\n",
      "Step: 472, x: 0.21640796959400177, loss: 7.7522\n",
      "Step: 473, x: 0.21709534525871277, loss: 7.7484\n",
      "Step: 474, x: 0.21778345108032227, loss: 7.7446\n",
      "Step: 475, x: 0.21847227215766907, loss: 7.7407\n",
      "Step: 476, x: 0.21916182339191437, loss: 7.7369\n",
      "Step: 477, x: 0.21985208988189697, loss: 7.7331\n",
      "Step: 478, x: 0.22054308652877808, loss: 7.7292\n",
      "Step: 479, x: 0.22123479843139648, loss: 7.7254\n",
      "Step: 480, x: 0.2219272404909134, loss: 7.7215\n",
      "Step: 481, x: 0.2226203978061676, loss: 7.7177\n",
      "Step: 482, x: 0.2233142852783203, loss: 7.7138\n",
      "Step: 483, x: 0.22400888800621033, loss: 7.7100\n",
      "Step: 484, x: 0.22470420598983765, loss: 7.7061\n",
      "Step: 485, x: 0.22540023922920227, loss: 7.7023\n",
      "Step: 486, x: 0.2260969877243042, loss: 7.6984\n",
      "Step: 487, x: 0.22679445147514343, loss: 7.6945\n",
      "Step: 488, x: 0.22749263048171997, loss: 7.6907\n",
      "Step: 489, x: 0.2281915247440338, loss: 7.6868\n",
      "Step: 490, x: 0.22889113426208496, loss: 7.6829\n",
      "Step: 491, x: 0.2295914590358734, loss: 7.6790\n",
      "Step: 492, x: 0.23029249906539917, loss: 7.6752\n",
      "Step: 493, x: 0.23099425435066223, loss: 7.6713\n",
      "Step: 494, x: 0.2316967248916626, loss: 7.6674\n",
      "Step: 495, x: 0.23239989578723907, loss: 7.6635\n",
      "Step: 496, x: 0.23310378193855286, loss: 7.6596\n",
      "Step: 497, x: 0.23380836844444275, loss: 7.6557\n",
      "Step: 498, x: 0.23451367020606995, loss: 7.6518\n",
      "Step: 499, x: 0.23521967232227325, loss: 7.6479\n",
      "Step: 500, x: 0.23592638969421387, loss: 7.6440\n",
      "Step: 501, x: 0.2366338074207306, loss: 7.6401\n",
      "Step: 502, x: 0.23734194040298462, loss: 7.6362\n",
      "Step: 503, x: 0.23805077373981476, loss: 7.6323\n",
      "Step: 504, x: 0.238760307431221, loss: 7.6284\n",
      "Step: 505, x: 0.23947054147720337, loss: 7.6244\n",
      "Step: 506, x: 0.24018149077892303, loss: 7.6205\n",
      "Step: 507, x: 0.2408931404352188, loss: 7.6166\n",
      "Step: 508, x: 0.2416054904460907, loss: 7.6127\n",
      "Step: 509, x: 0.2423185408115387, loss: 7.6087\n",
      "Step: 510, x: 0.2430322915315628, loss: 7.6048\n",
      "Step: 511, x: 0.24374674260616302, loss: 7.6009\n",
      "Step: 512, x: 0.24446189403533936, loss: 7.5969\n",
      "Step: 513, x: 0.2451777309179306, loss: 7.5930\n",
      "Step: 514, x: 0.24589426815509796, loss: 7.5890\n",
      "Step: 515, x: 0.24661150574684143, loss: 7.5851\n",
      "Step: 516, x: 0.247329443693161, loss: 7.5811\n",
      "Step: 517, x: 0.2480480670928955, loss: 7.5772\n",
      "Step: 518, x: 0.24876739084720612, loss: 7.5732\n",
      "Step: 519, x: 0.24948740005493164, loss: 7.5693\n",
      "Step: 520, x: 0.2502081096172333, loss: 7.5653\n",
      "Step: 521, x: 0.25092950463294983, loss: 7.5614\n",
      "Step: 522, x: 0.2516515851020813, loss: 7.5574\n",
      "Step: 523, x: 0.2523743808269501, loss: 7.5534\n",
      "Step: 524, x: 0.25309786200523376, loss: 7.5494\n",
      "Step: 525, x: 0.2538220286369324, loss: 7.5455\n",
      "Step: 526, x: 0.2545468807220459, loss: 7.5415\n",
      "Step: 527, x: 0.25527241826057434, loss: 7.5375\n",
      "Step: 528, x: 0.2559986412525177, loss: 7.5335\n",
      "Step: 529, x: 0.256725549697876, loss: 7.5295\n",
      "Step: 530, x: 0.25745314359664917, loss: 7.5256\n",
      "Step: 531, x: 0.2581814229488373, loss: 7.5216\n",
      "Step: 532, x: 0.2589103877544403, loss: 7.5176\n",
      "Step: 533, x: 0.25964003801345825, loss: 7.5136\n",
      "Step: 534, x: 0.2603703737258911, loss: 7.5096\n",
      "Step: 535, x: 0.2611013948917389, loss: 7.5056\n",
      "Step: 536, x: 0.2618331015110016, loss: 7.5016\n",
      "Step: 537, x: 0.2625654935836792, loss: 7.4976\n",
      "Step: 538, x: 0.26329857110977173, loss: 7.4935\n",
      "Step: 539, x: 0.2640323340892792, loss: 7.4895\n",
      "Step: 540, x: 0.26476678252220154, loss: 7.4855\n",
      "Step: 541, x: 0.26550188660621643, loss: 7.4815\n",
      "Step: 542, x: 0.26623767614364624, loss: 7.4775\n",
      "Step: 543, x: 0.26697415113449097, loss: 7.4735\n",
      "Step: 544, x: 0.2677113115787506, loss: 7.4694\n",
      "Step: 545, x: 0.2684491276741028, loss: 7.4654\n",
      "Step: 546, x: 0.2691876292228699, loss: 7.4614\n",
      "Step: 547, x: 0.2699268162250519, loss: 7.4573\n",
      "Step: 548, x: 0.2706666588783264, loss: 7.4533\n",
      "Step: 549, x: 0.27140718698501587, loss: 7.4493\n",
      "Step: 550, x: 0.27214840054512024, loss: 7.4452\n",
      "Step: 551, x: 0.27289026975631714, loss: 7.4412\n",
      "Step: 552, x: 0.27363282442092896, loss: 7.4371\n",
      "Step: 553, x: 0.2743760347366333, loss: 7.4331\n",
      "Step: 554, x: 0.27511993050575256, loss: 7.4290\n",
      "Step: 555, x: 0.27586448192596436, loss: 7.4250\n",
      "Step: 556, x: 0.27660971879959106, loss: 7.4209\n",
      "Step: 557, x: 0.2773556113243103, loss: 7.4169\n",
      "Step: 558, x: 0.27810218930244446, loss: 7.4128\n",
      "Step: 559, x: 0.27884942293167114, loss: 7.4087\n",
      "Step: 560, x: 0.27959734201431274, loss: 7.4047\n",
      "Step: 561, x: 0.2803459167480469, loss: 7.4006\n",
      "Step: 562, x: 0.2810951769351959, loss: 7.3965\n",
      "Step: 563, x: 0.2818450927734375, loss: 7.3924\n",
      "Step: 564, x: 0.2825956642627716, loss: 7.3884\n",
      "Step: 565, x: 0.28334692120552063, loss: 7.3843\n",
      "Step: 566, x: 0.2840988337993622, loss: 7.3802\n",
      "Step: 567, x: 0.28485140204429626, loss: 7.3761\n",
      "Step: 568, x: 0.2856046259403229, loss: 7.3720\n",
      "Step: 569, x: 0.2863585352897644, loss: 7.3679\n",
      "Step: 570, x: 0.28711310029029846, loss: 7.3638\n",
      "Step: 571, x: 0.28786832094192505, loss: 7.3598\n",
      "Step: 572, x: 0.28862419724464417, loss: 7.3557\n",
      "Step: 573, x: 0.2893807291984558, loss: 7.3516\n",
      "Step: 574, x: 0.29013791680336, loss: 7.3475\n",
      "Step: 575, x: 0.2908957898616791, loss: 7.3434\n",
      "Step: 576, x: 0.2916543185710907, loss: 7.3392\n",
      "Step: 577, x: 0.29241350293159485, loss: 7.3351\n",
      "Step: 578, x: 0.29317334294319153, loss: 7.3310\n",
      "Step: 579, x: 0.29393383860588074, loss: 7.3269\n",
      "Step: 580, x: 0.2946949899196625, loss: 7.3228\n",
      "Step: 581, x: 0.29545679688453674, loss: 7.3187\n",
      "Step: 582, x: 0.29621925950050354, loss: 7.3146\n",
      "Step: 583, x: 0.29698237776756287, loss: 7.3104\n",
      "Step: 584, x: 0.2977461516857147, loss: 7.3063\n",
      "Step: 585, x: 0.2985105812549591, loss: 7.3022\n",
      "Step: 586, x: 0.299275666475296, loss: 7.2980\n",
      "Step: 587, x: 0.3000413775444031, loss: 7.2939\n",
      "Step: 588, x: 0.30080774426460266, loss: 7.2898\n",
      "Step: 589, x: 0.3015747666358948, loss: 7.2856\n",
      "Step: 590, x: 0.3023424446582794, loss: 7.2815\n",
      "Step: 591, x: 0.3031107783317566, loss: 7.2774\n",
      "Step: 592, x: 0.3038797676563263, loss: 7.2732\n",
      "Step: 593, x: 0.30464938282966614, loss: 7.2691\n",
      "Step: 594, x: 0.3054196536540985, loss: 7.2649\n",
      "Step: 595, x: 0.3061905801296234, loss: 7.2608\n",
      "Step: 596, x: 0.30696216225624084, loss: 7.2566\n",
      "Step: 597, x: 0.3077343702316284, loss: 7.2525\n",
      "Step: 598, x: 0.3085072338581085, loss: 7.2483\n",
      "Step: 599, x: 0.30928075313568115, loss: 7.2441\n",
      "Step: 600, x: 0.3100548982620239, loss: 7.2400\n",
      "Step: 601, x: 0.31082969903945923, loss: 7.2358\n",
      "Step: 602, x: 0.31160515546798706, loss: 7.2316\n",
      "Step: 603, x: 0.31238123774528503, loss: 7.2275\n",
      "Step: 604, x: 0.31315797567367554, loss: 7.2233\n",
      "Step: 605, x: 0.3139353394508362, loss: 7.2191\n",
      "Step: 606, x: 0.31471335887908936, loss: 7.2149\n",
      "Step: 607, x: 0.31549200415611267, loss: 7.2108\n",
      "Step: 608, x: 0.3162713050842285, loss: 7.2066\n",
      "Step: 609, x: 0.3170512318611145, loss: 7.2024\n",
      "Step: 610, x: 0.317831814289093, loss: 7.1982\n",
      "Step: 611, x: 0.3186130225658417, loss: 7.1940\n",
      "Step: 612, x: 0.31939488649368286, loss: 7.1898\n",
      "Step: 613, x: 0.3201773762702942, loss: 7.1856\n",
      "Step: 614, x: 0.32096052169799805, loss: 7.1814\n",
      "Step: 615, x: 0.32174429297447205, loss: 7.1773\n",
      "Step: 616, x: 0.3225286900997162, loss: 7.1731\n",
      "Step: 617, x: 0.32331374287605286, loss: 7.1689\n",
      "Step: 618, x: 0.32409942150115967, loss: 7.1646\n",
      "Step: 619, x: 0.3248857259750366, loss: 7.1604\n",
      "Step: 620, x: 0.3256726861000061, loss: 7.1562\n",
      "Step: 621, x: 0.3264602720737457, loss: 7.1520\n",
      "Step: 622, x: 0.3272484838962555, loss: 7.1478\n",
      "Step: 623, x: 0.3280373215675354, loss: 7.1436\n",
      "Step: 624, x: 0.32882681488990784, loss: 7.1394\n",
      "Step: 625, x: 0.3296169340610504, loss: 7.1352\n",
      "Step: 626, x: 0.33040767908096313, loss: 7.1309\n",
      "Step: 627, x: 0.331199049949646, loss: 7.1267\n",
      "Step: 628, x: 0.331991046667099, loss: 7.1225\n",
      "Step: 629, x: 0.33278369903564453, loss: 7.1183\n",
      "Step: 630, x: 0.3335769772529602, loss: 7.1140\n",
      "Step: 631, x: 0.334370881319046, loss: 7.1098\n",
      "Step: 632, x: 0.335165411233902, loss: 7.1056\n",
      "Step: 633, x: 0.3359605669975281, loss: 7.1013\n",
      "Step: 634, x: 0.3367563486099243, loss: 7.0971\n",
      "Step: 635, x: 0.3375527560710907, loss: 7.0929\n",
      "Step: 636, x: 0.3383497893810272, loss: 7.0886\n",
      "Step: 637, x: 0.3391474485397339, loss: 7.0844\n",
      "Step: 638, x: 0.3399457335472107, loss: 7.0801\n",
      "Step: 639, x: 0.34074464440345764, loss: 7.0759\n",
      "Step: 640, x: 0.34154418110847473, loss: 7.0716\n",
      "Step: 641, x: 0.34234434366226196, loss: 7.0674\n",
      "Step: 642, x: 0.34314513206481934, loss: 7.0631\n",
      "Step: 643, x: 0.34394654631614685, loss: 7.0589\n",
      "Step: 644, x: 0.3447485864162445, loss: 7.0546\n",
      "Step: 645, x: 0.3455512523651123, loss: 7.0504\n",
      "Step: 646, x: 0.34635454416275024, loss: 7.0461\n",
      "Step: 647, x: 0.34715843200683594, loss: 7.0418\n",
      "Step: 648, x: 0.3479629456996918, loss: 7.0376\n",
      "Step: 649, x: 0.34876808524131775, loss: 7.0333\n",
      "Step: 650, x: 0.34957385063171387, loss: 7.0290\n",
      "Step: 651, x: 0.3503802418708801, loss: 7.0248\n",
      "Step: 652, x: 0.35118722915649414, loss: 7.0205\n",
      "Step: 653, x: 0.3519948422908783, loss: 7.0162\n",
      "Step: 654, x: 0.3528030812740326, loss: 7.0119\n",
      "Step: 655, x: 0.35361194610595703, loss: 7.0077\n",
      "Step: 656, x: 0.3544214069843292, loss: 7.0034\n",
      "Step: 657, x: 0.35523149371147156, loss: 6.9991\n",
      "Step: 658, x: 0.35604220628738403, loss: 6.9948\n",
      "Step: 659, x: 0.35685351490974426, loss: 6.9905\n",
      "Step: 660, x: 0.35766544938087463, loss: 6.9862\n",
      "Step: 661, x: 0.35847800970077515, loss: 6.9819\n",
      "Step: 662, x: 0.3592911660671234, loss: 6.9776\n",
      "Step: 663, x: 0.3601049482822418, loss: 6.9733\n",
      "Step: 664, x: 0.360919326543808, loss: 6.9690\n",
      "Step: 665, x: 0.3617343306541443, loss: 6.9647\n",
      "Step: 666, x: 0.36254993081092834, loss: 6.9604\n",
      "Step: 667, x: 0.36336615681648254, loss: 6.9561\n",
      "Step: 668, x: 0.3641830086708069, loss: 6.9518\n",
      "Step: 669, x: 0.365000456571579, loss: 6.9475\n",
      "Step: 670, x: 0.3658185303211212, loss: 6.9432\n",
      "Step: 671, x: 0.3666372001171112, loss: 6.9389\n",
      "Step: 672, x: 0.36745646595954895, loss: 6.9346\n",
      "Step: 673, x: 0.36827635765075684, loss: 6.9303\n",
      "Step: 674, x: 0.3690968453884125, loss: 6.9260\n",
      "Step: 675, x: 0.36991795897483826, loss: 6.9217\n",
      "Step: 676, x: 0.3707396686077118, loss: 6.9173\n",
      "Step: 677, x: 0.37156200408935547, loss: 6.9130\n",
      "Step: 678, x: 0.3723849356174469, loss: 6.9087\n",
      "Step: 679, x: 0.3732084631919861, loss: 6.9044\n",
      "Step: 680, x: 0.3740326166152954, loss: 6.9000\n",
      "Step: 681, x: 0.3748573660850525, loss: 6.8957\n",
      "Step: 682, x: 0.3756827116012573, loss: 6.8914\n",
      "Step: 683, x: 0.3765086531639099, loss: 6.8870\n",
      "Step: 684, x: 0.37733522057533264, loss: 6.8827\n",
      "Step: 685, x: 0.3781623840332031, loss: 6.8784\n",
      "Step: 686, x: 0.37899014353752136, loss: 6.8740\n",
      "Step: 687, x: 0.37981849908828735, loss: 6.8697\n",
      "Step: 688, x: 0.3806474804878235, loss: 6.8654\n",
      "Step: 689, x: 0.3814770579338074, loss: 6.8610\n",
      "Step: 690, x: 0.382307231426239, loss: 6.8567\n",
      "Step: 691, x: 0.3831380009651184, loss: 6.8523\n",
      "Step: 692, x: 0.38396936655044556, loss: 6.8480\n",
      "Step: 693, x: 0.38480135798454285, loss: 6.8436\n",
      "Step: 694, x: 0.3856339454650879, loss: 6.8393\n",
      "Step: 695, x: 0.3864671289920807, loss: 6.8349\n",
      "Step: 696, x: 0.38730090856552124, loss: 6.8306\n",
      "Step: 697, x: 0.38813528418540955, loss: 6.8262\n",
      "Step: 698, x: 0.3889702558517456, loss: 6.8218\n",
      "Step: 699, x: 0.3898058235645294, loss: 6.8175\n",
      "Step: 700, x: 0.390641987323761, loss: 6.8131\n",
      "Step: 701, x: 0.3914787471294403, loss: 6.8087\n",
      "Step: 702, x: 0.3923161029815674, loss: 6.8044\n",
      "Step: 703, x: 0.3931540548801422, loss: 6.8000\n",
      "Step: 704, x: 0.3939926028251648, loss: 6.7956\n",
      "Step: 705, x: 0.39483174681663513, loss: 6.7913\n",
      "Step: 706, x: 0.3956714868545532, loss: 6.7869\n",
      "Step: 707, x: 0.39651182293891907, loss: 6.7825\n",
      "Step: 708, x: 0.39735275506973267, loss: 6.7782\n",
      "Step: 709, x: 0.398194283246994, loss: 6.7738\n",
      "Step: 710, x: 0.3990364074707031, loss: 6.7694\n",
      "Step: 711, x: 0.39987912774086, loss: 6.7650\n",
      "Step: 712, x: 0.4007224440574646, loss: 6.7606\n",
      "Step: 713, x: 0.4015663266181946, loss: 6.7562\n",
      "Step: 714, x: 0.4024108052253723, loss: 6.7519\n",
      "Step: 715, x: 0.4032558798789978, loss: 6.7475\n",
      "Step: 716, x: 0.40410155057907104, loss: 6.7431\n",
      "Step: 717, x: 0.40494781732559204, loss: 6.7387\n",
      "Step: 718, x: 0.4057946503162384, loss: 6.7343\n",
      "Step: 719, x: 0.4066420793533325, loss: 6.7299\n",
      "Step: 720, x: 0.4074901044368744, loss: 6.7255\n",
      "Step: 721, x: 0.408338725566864, loss: 6.7211\n",
      "Step: 722, x: 0.4091879427433014, loss: 6.7167\n",
      "Step: 723, x: 0.41003772616386414, loss: 6.7123\n",
      "Step: 724, x: 0.41088810563087463, loss: 6.7079\n",
      "Step: 725, x: 0.4117390811443329, loss: 6.7035\n",
      "Step: 726, x: 0.4125906229019165, loss: 6.6991\n",
      "Step: 727, x: 0.4134427607059479, loss: 6.6947\n",
      "Step: 728, x: 0.414295494556427, loss: 6.6903\n",
      "Step: 729, x: 0.4151487946510315, loss: 6.6859\n",
      "Step: 730, x: 0.41600269079208374, loss: 6.6815\n",
      "Step: 731, x: 0.41685718297958374, loss: 6.6770\n",
      "Step: 732, x: 0.4177122414112091, loss: 6.6726\n",
      "Step: 733, x: 0.4185678958892822, loss: 6.6682\n",
      "Step: 734, x: 0.4194241166114807, loss: 6.6638\n",
      "Step: 735, x: 0.42028093338012695, loss: 6.6594\n",
      "Step: 736, x: 0.42113831639289856, loss: 6.6550\n",
      "Step: 737, x: 0.4219962954521179, loss: 6.6505\n",
      "Step: 738, x: 0.42285484075546265, loss: 6.6461\n",
      "Step: 739, x: 0.4237139821052551, loss: 6.6417\n",
      "Step: 740, x: 0.424573689699173, loss: 6.6372\n",
      "Step: 741, x: 0.4254339933395386, loss: 6.6328\n",
      "Step: 742, x: 0.42629486322402954, loss: 6.6284\n",
      "Step: 743, x: 0.42715632915496826, loss: 6.6240\n",
      "Step: 744, x: 0.42801836133003235, loss: 6.6195\n",
      "Step: 745, x: 0.4288809895515442, loss: 6.6151\n",
      "Step: 746, x: 0.4297441840171814, loss: 6.6107\n",
      "Step: 747, x: 0.43060794472694397, loss: 6.6062\n",
      "Step: 748, x: 0.4314723014831543, loss: 6.6018\n",
      "Step: 749, x: 0.43233722448349, loss: 6.5973\n",
      "Step: 750, x: 0.43320274353027344, loss: 6.5929\n",
      "Step: 751, x: 0.43406882882118225, loss: 6.5884\n",
      "Step: 752, x: 0.43493548035621643, loss: 6.5840\n",
      "Step: 753, x: 0.43580272793769836, loss: 6.5796\n",
      "Step: 754, x: 0.43667054176330566, loss: 6.5751\n",
      "Step: 755, x: 0.43753892183303833, loss: 6.5707\n",
      "Step: 756, x: 0.43840789794921875, loss: 6.5662\n",
      "Step: 757, x: 0.43927744030952454, loss: 6.5618\n",
      "Step: 758, x: 0.4401475489139557, loss: 6.5573\n",
      "Step: 759, x: 0.4410182237625122, loss: 6.5528\n",
      "Step: 760, x: 0.4418894946575165, loss: 6.5484\n",
      "Step: 761, x: 0.4427613317966461, loss: 6.5439\n",
      "Step: 762, x: 0.4436337351799011, loss: 6.5395\n",
      "Step: 763, x: 0.4445067048072815, loss: 6.5350\n",
      "Step: 764, x: 0.44538024067878723, loss: 6.5305\n",
      "Step: 765, x: 0.44625434279441833, loss: 6.5261\n",
      "Step: 766, x: 0.4471290409564972, loss: 6.5216\n",
      "Step: 767, x: 0.4480043053627014, loss: 6.5172\n",
      "Step: 768, x: 0.448880136013031, loss: 6.5127\n",
      "Step: 769, x: 0.44975653290748596, loss: 6.5082\n",
      "Step: 770, x: 0.4506334960460663, loss: 6.5037\n",
      "Step: 771, x: 0.451511025428772, loss: 6.4993\n",
      "Step: 772, x: 0.452389121055603, loss: 6.4948\n",
      "Step: 773, x: 0.45326778292655945, loss: 6.4903\n",
      "Step: 774, x: 0.45414701104164124, loss: 6.4858\n",
      "Step: 775, x: 0.4550268352031708, loss: 6.4814\n",
      "Step: 776, x: 0.4559072256088257, loss: 6.4769\n",
      "Step: 777, x: 0.45678818225860596, loss: 6.4724\n",
      "Step: 778, x: 0.4576697051525116, loss: 6.4679\n",
      "Step: 779, x: 0.4585517942905426, loss: 6.4634\n",
      "Step: 780, x: 0.459434449672699, loss: 6.4590\n",
      "Step: 781, x: 0.4603176712989807, loss: 6.4545\n",
      "Step: 782, x: 0.46120142936706543, loss: 6.4500\n",
      "Step: 783, x: 0.4620857536792755, loss: 6.4455\n",
      "Step: 784, x: 0.46297064423561096, loss: 6.4410\n",
      "Step: 785, x: 0.4638561010360718, loss: 6.4365\n",
      "Step: 786, x: 0.46474212408065796, loss: 6.4320\n",
      "Step: 787, x: 0.4656287133693695, loss: 6.4275\n",
      "Step: 788, x: 0.4665158689022064, loss: 6.4230\n",
      "Step: 789, x: 0.4674035906791687, loss: 6.4185\n",
      "Step: 790, x: 0.46829187870025635, loss: 6.4140\n",
      "Step: 791, x: 0.46918073296546936, loss: 6.4095\n",
      "Step: 792, x: 0.47007012367248535, loss: 6.4050\n",
      "Step: 793, x: 0.4709600806236267, loss: 6.4005\n",
      "Step: 794, x: 0.47185060381889343, loss: 6.3960\n",
      "Step: 795, x: 0.4727416932582855, loss: 6.3915\n",
      "Step: 796, x: 0.473633348941803, loss: 6.3870\n",
      "Step: 797, x: 0.4745255410671234, loss: 6.3825\n",
      "Step: 798, x: 0.4754182994365692, loss: 6.3780\n",
      "Step: 799, x: 0.4763116240501404, loss: 6.3735\n",
      "Step: 800, x: 0.4772055149078369, loss: 6.3690\n",
      "Step: 801, x: 0.4780999422073364, loss: 6.3645\n",
      "Step: 802, x: 0.4789949357509613, loss: 6.3600\n",
      "Step: 803, x: 0.47989049553871155, loss: 6.3555\n",
      "Step: 804, x: 0.48078662157058716, loss: 6.3510\n",
      "Step: 805, x: 0.48168328404426575, loss: 6.3464\n",
      "Step: 806, x: 0.4825805127620697, loss: 6.3419\n",
      "Step: 807, x: 0.483478307723999, loss: 6.3374\n",
      "Step: 808, x: 0.4843766391277313, loss: 6.3329\n",
      "Step: 809, x: 0.485275536775589, loss: 6.3284\n",
      "Step: 810, x: 0.486175000667572, loss: 6.3238\n",
      "Step: 811, x: 0.48707500100135803, loss: 6.3193\n",
      "Step: 812, x: 0.4879755675792694, loss: 6.3148\n",
      "Step: 813, x: 0.48887667059898376, loss: 6.3103\n",
      "Step: 814, x: 0.4897783398628235, loss: 6.3057\n",
      "Step: 815, x: 0.4906805455684662, loss: 6.3012\n",
      "Step: 816, x: 0.49158331751823425, loss: 6.2967\n",
      "Step: 817, x: 0.4924866557121277, loss: 6.2922\n",
      "Step: 818, x: 0.4933905303478241, loss: 6.2876\n",
      "Step: 819, x: 0.4942949712276459, loss: 6.2831\n",
      "Step: 820, x: 0.49519994854927063, loss: 6.2786\n",
      "Step: 821, x: 0.49610549211502075, loss: 6.2740\n",
      "Step: 822, x: 0.49701157212257385, loss: 6.2695\n",
      "Step: 823, x: 0.4979182183742523, loss: 6.2650\n",
      "Step: 824, x: 0.49882540106773376, loss: 6.2604\n",
      "Step: 825, x: 0.4997331500053406, loss: 6.2559\n",
      "Step: 826, x: 0.5006414651870728, loss: 6.2513\n",
      "Step: 827, x: 0.5015503168106079, loss: 6.2468\n",
      "Step: 828, x: 0.502459704875946, loss: 6.2423\n",
      "Step: 829, x: 0.5033696293830872, loss: 6.2377\n",
      "Step: 830, x: 0.5042800903320312, loss: 6.2332\n",
      "Step: 831, x: 0.5051911473274231, loss: 6.2286\n",
      "Step: 832, x: 0.5061027407646179, loss: 6.2241\n",
      "Step: 833, x: 0.5070148706436157, loss: 6.2195\n",
      "Step: 834, x: 0.5079275369644165, loss: 6.2150\n",
      "Step: 835, x: 0.5088407397270203, loss: 6.2104\n",
      "Step: 836, x: 0.509754478931427, loss: 6.2059\n",
      "Step: 837, x: 0.5106688141822815, loss: 6.2013\n",
      "Step: 838, x: 0.511583685874939, loss: 6.1968\n",
      "Step: 839, x: 0.5124990940093994, loss: 6.1922\n",
      "Step: 840, x: 0.5134150385856628, loss: 6.1877\n",
      "Step: 841, x: 0.5143315196037292, loss: 6.1831\n",
      "Step: 842, x: 0.5152485370635986, loss: 6.1785\n",
      "Step: 843, x: 0.516166090965271, loss: 6.1740\n",
      "Step: 844, x: 0.5170842409133911, loss: 6.1694\n",
      "Step: 845, x: 0.5180029273033142, loss: 6.1649\n",
      "Step: 846, x: 0.5189221501350403, loss: 6.1603\n",
      "Step: 847, x: 0.5198419094085693, loss: 6.1557\n",
      "Step: 848, x: 0.5207622051239014, loss: 6.1512\n",
      "Step: 849, x: 0.5216830372810364, loss: 6.1466\n",
      "Step: 850, x: 0.5226044058799744, loss: 6.1421\n",
      "Step: 851, x: 0.5235263109207153, loss: 6.1375\n",
      "Step: 852, x: 0.5244487524032593, loss: 6.1329\n",
      "Step: 853, x: 0.5253717303276062, loss: 6.1284\n",
      "Step: 854, x: 0.5262953042984009, loss: 6.1238\n",
      "Step: 855, x: 0.5272194147109985, loss: 6.1192\n",
      "Step: 856, x: 0.5281440615653992, loss: 6.1146\n",
      "Step: 857, x: 0.5290692448616028, loss: 6.1101\n",
      "Step: 858, x: 0.5299949645996094, loss: 6.1055\n",
      "Step: 859, x: 0.530921220779419, loss: 6.1009\n",
      "Step: 860, x: 0.5318480134010315, loss: 6.0964\n",
      "Step: 861, x: 0.532775342464447, loss: 6.0918\n",
      "Step: 862, x: 0.5337032079696655, loss: 6.0872\n",
      "Step: 863, x: 0.534631609916687, loss: 6.0826\n",
      "Step: 864, x: 0.5355605483055115, loss: 6.0780\n",
      "Step: 865, x: 0.5364900231361389, loss: 6.0735\n",
      "Step: 866, x: 0.5374200344085693, loss: 6.0689\n",
      "Step: 867, x: 0.5383505821228027, loss: 6.0643\n",
      "Step: 868, x: 0.5392816662788391, loss: 6.0597\n",
      "Step: 869, x: 0.5402132868766785, loss: 6.0551\n",
      "Step: 870, x: 0.5411454439163208, loss: 6.0506\n",
      "Step: 871, x: 0.5420781373977661, loss: 6.0460\n",
      "Step: 872, x: 0.5430113673210144, loss: 6.0414\n",
      "Step: 873, x: 0.5439451336860657, loss: 6.0368\n",
      "Step: 874, x: 0.5448794364929199, loss: 6.0322\n",
      "Step: 875, x: 0.5458142757415771, loss: 6.0276\n",
      "Step: 876, x: 0.5467496514320374, loss: 6.0230\n",
      "Step: 877, x: 0.5476855635643005, loss: 6.0184\n",
      "Step: 878, x: 0.5486220121383667, loss: 6.0138\n",
      "Step: 879, x: 0.5495589971542358, loss: 6.0093\n",
      "Step: 880, x: 0.550496518611908, loss: 6.0047\n",
      "Step: 881, x: 0.5514345169067383, loss: 6.0001\n",
      "Step: 882, x: 0.5523730516433716, loss: 5.9955\n",
      "Step: 883, x: 0.5533121228218079, loss: 5.9909\n",
      "Step: 884, x: 0.5542517304420471, loss: 5.9863\n",
      "Step: 885, x: 0.5551918745040894, loss: 5.9817\n",
      "Step: 886, x: 0.5561325550079346, loss: 5.9771\n",
      "Step: 887, x: 0.5570737719535828, loss: 5.9725\n",
      "Step: 888, x: 0.5580155253410339, loss: 5.9679\n",
      "Step: 889, x: 0.5589578151702881, loss: 5.9633\n",
      "Step: 890, x: 0.5599006414413452, loss: 5.9587\n",
      "Step: 891, x: 0.5608439445495605, loss: 5.9541\n",
      "Step: 892, x: 0.5617877840995789, loss: 5.9495\n",
      "Step: 893, x: 0.5627321600914001, loss: 5.9449\n",
      "Step: 894, x: 0.5636770725250244, loss: 5.9403\n",
      "Step: 895, x: 0.5646225214004517, loss: 5.9357\n",
      "Step: 896, x: 0.5655685067176819, loss: 5.9311\n",
      "Step: 897, x: 0.5665150284767151, loss: 5.9265\n",
      "Step: 898, x: 0.5674620866775513, loss: 5.9218\n",
      "Step: 899, x: 0.5684096217155457, loss: 5.9172\n",
      "Step: 900, x: 0.569357693195343, loss: 5.9126\n",
      "Step: 901, x: 0.5703063011169434, loss: 5.9080\n",
      "Step: 902, x: 0.5712554454803467, loss: 5.9034\n",
      "Step: 903, x: 0.572205126285553, loss: 5.8988\n",
      "Step: 904, x: 0.5731552839279175, loss: 5.8942\n",
      "Step: 905, x: 0.574105978012085, loss: 5.8896\n",
      "Step: 906, x: 0.5750572085380554, loss: 5.8850\n",
      "Step: 907, x: 0.5760089755058289, loss: 5.8803\n",
      "Step: 908, x: 0.5769612789154053, loss: 5.8757\n",
      "Step: 909, x: 0.5779140591621399, loss: 5.8711\n",
      "Step: 910, x: 0.5788673758506775, loss: 5.8665\n",
      "Step: 911, x: 0.5798212289810181, loss: 5.8619\n",
      "Step: 912, x: 0.5807756185531616, loss: 5.8573\n",
      "Step: 913, x: 0.5817305445671082, loss: 5.8526\n",
      "Step: 914, x: 0.5826859474182129, loss: 5.8480\n",
      "Step: 915, x: 0.5836418867111206, loss: 5.8434\n",
      "Step: 916, x: 0.5845983624458313, loss: 5.8388\n",
      "Step: 917, x: 0.585555374622345, loss: 5.8342\n",
      "Step: 918, x: 0.5865128636360168, loss: 5.8295\n",
      "Step: 919, x: 0.5874708890914917, loss: 5.8249\n",
      "Step: 920, x: 0.5884294509887695, loss: 5.8203\n",
      "Step: 921, x: 0.5893885493278503, loss: 5.8157\n",
      "Step: 922, x: 0.5903481245040894, loss: 5.8110\n",
      "Step: 923, x: 0.5913082361221313, loss: 5.8064\n",
      "Step: 924, x: 0.5922688841819763, loss: 5.8018\n",
      "Step: 925, x: 0.5932300090789795, loss: 5.7972\n",
      "Step: 926, x: 0.5941916704177856, loss: 5.7925\n",
      "Step: 927, x: 0.5951538681983948, loss: 5.7879\n",
      "Step: 928, x: 0.5961166024208069, loss: 5.7833\n",
      "Step: 929, x: 0.5970798134803772, loss: 5.7787\n",
      "Step: 930, x: 0.5980435609817505, loss: 5.7740\n",
      "Step: 931, x: 0.5990078449249268, loss: 5.7694\n",
      "Step: 932, x: 0.5999726057052612, loss: 5.7648\n",
      "Step: 933, x: 0.6009379029273987, loss: 5.7601\n",
      "Step: 934, x: 0.6019037365913391, loss: 5.7555\n",
      "Step: 935, x: 0.6028700470924377, loss: 5.7509\n",
      "Step: 936, x: 0.6038368940353394, loss: 5.7462\n",
      "Step: 937, x: 0.604804277420044, loss: 5.7416\n",
      "Step: 938, x: 0.6057721376419067, loss: 5.7370\n",
      "Step: 939, x: 0.6067405343055725, loss: 5.7323\n",
      "Step: 940, x: 0.6077094674110413, loss: 5.7277\n",
      "Step: 941, x: 0.6086788773536682, loss: 5.7231\n",
      "Step: 942, x: 0.6096488237380981, loss: 5.7184\n",
      "Step: 943, x: 0.610619306564331, loss: 5.7138\n",
      "Step: 944, x: 0.6115902662277222, loss: 5.7091\n",
      "Step: 945, x: 0.6125617623329163, loss: 5.7045\n",
      "Step: 946, x: 0.6135337352752686, loss: 5.6999\n",
      "Step: 947, x: 0.6145062446594238, loss: 5.6952\n",
      "Step: 948, x: 0.6154792904853821, loss: 5.6906\n",
      "Step: 949, x: 0.6164528131484985, loss: 5.6859\n",
      "Step: 950, x: 0.617426872253418, loss: 5.6813\n",
      "Step: 951, x: 0.6184014081954956, loss: 5.6767\n",
      "Step: 952, x: 0.6193764805793762, loss: 5.6720\n",
      "Step: 953, x: 0.6203520894050598, loss: 5.6674\n",
      "Step: 954, x: 0.6213281750679016, loss: 5.6627\n",
      "Step: 955, x: 0.6223047971725464, loss: 5.6581\n",
      "Step: 956, x: 0.6232818961143494, loss: 5.6534\n",
      "Step: 957, x: 0.6242595314979553, loss: 5.6488\n",
      "Step: 958, x: 0.6252376437187195, loss: 5.6441\n",
      "Step: 959, x: 0.6262162923812866, loss: 5.6395\n",
      "Step: 960, x: 0.6271954774856567, loss: 5.6348\n",
      "Step: 961, x: 0.6281751394271851, loss: 5.6302\n",
      "Step: 962, x: 0.6291553378105164, loss: 5.6256\n",
      "Step: 963, x: 0.6301360130310059, loss: 5.6209\n",
      "Step: 964, x: 0.6311172246932983, loss: 5.6163\n",
      "Step: 965, x: 0.632098913192749, loss: 5.6116\n",
      "Step: 966, x: 0.6330811381340027, loss: 5.6070\n",
      "Step: 967, x: 0.6340638399124146, loss: 5.6023\n",
      "Step: 968, x: 0.6350470781326294, loss: 5.5977\n",
      "Step: 969, x: 0.6360307931900024, loss: 5.5930\n",
      "Step: 970, x: 0.6370150446891785, loss: 5.5884\n",
      "Step: 971, x: 0.6379997730255127, loss: 5.5837\n",
      "Step: 972, x: 0.6389850378036499, loss: 5.5790\n",
      "Step: 973, x: 0.6399707794189453, loss: 5.5744\n",
      "Step: 974, x: 0.6409570574760437, loss: 5.5697\n",
      "Step: 975, x: 0.6419438123703003, loss: 5.5651\n",
      "Step: 976, x: 0.6429311037063599, loss: 5.5604\n",
      "Step: 977, x: 0.6439188718795776, loss: 5.5558\n",
      "Step: 978, x: 0.6449071764945984, loss: 5.5511\n",
      "Step: 979, x: 0.6458959579467773, loss: 5.5465\n",
      "Step: 980, x: 0.6468852758407593, loss: 5.5418\n",
      "Step: 981, x: 0.6478750705718994, loss: 5.5371\n",
      "Step: 982, x: 0.6488654017448425, loss: 5.5325\n",
      "Step: 983, x: 0.6498562097549438, loss: 5.5278\n",
      "Step: 984, x: 0.6508475542068481, loss: 5.5232\n",
      "Step: 985, x: 0.6518393754959106, loss: 5.5185\n",
      "Step: 986, x: 0.6528316736221313, loss: 5.5139\n",
      "Step: 987, x: 0.653824508190155, loss: 5.5092\n",
      "Step: 988, x: 0.6548178195953369, loss: 5.5045\n",
      "Step: 989, x: 0.6558116674423218, loss: 5.4999\n",
      "Step: 990, x: 0.6568059921264648, loss: 5.4952\n",
      "Step: 991, x: 0.6578008532524109, loss: 5.4906\n",
      "Step: 992, x: 0.6587961912155151, loss: 5.4859\n",
      "Step: 993, x: 0.6597920060157776, loss: 5.4812\n",
      "Step: 994, x: 0.660788357257843, loss: 5.4766\n",
      "Step: 995, x: 0.6617851853370667, loss: 5.4719\n",
      "Step: 996, x: 0.6627825498580933, loss: 5.4672\n",
      "Step: 997, x: 0.6637803912162781, loss: 5.4626\n",
      "Step: 998, x: 0.6647787094116211, loss: 5.4579\n",
      "Step: 999, x: 0.6657775640487671, loss: 5.4533\n",
      "Step: 1000, x: 0.6667768955230713, loss: 5.4486\n"
     ]
    }
   ],
   "source": [
    "x = Parameter(data=torch.tensor([0.0]), requires_grad=True)\n",
    "optimizer = AdaDelta(lr=100, params=[x], alpha=0.001)\n",
    "\n",
    "loss_threshold = 1e-3\n",
    "prev_loss = 0\n",
    "\n",
    "for step in range(1000):\n",
    "    loss = loss_fn(x)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Step: {step+1}, x: {x.item()}, loss: {loss.item():.4f}\")\n",
    "\n",
    "    # if abs(prev_loss - loss.item()) < loss_threshold:\n",
    "    #     print(f\"difference in loss is less than the threshold; stopping\")\n",
    "    #     break\n",
    "    # prev_loss = loss.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
